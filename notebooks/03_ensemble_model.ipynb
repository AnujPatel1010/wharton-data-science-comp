{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b1ee4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 28 features from original baseline:\n",
      "['home_xg', 'away_xg', 'home_shots', 'away_shots', 'home_penalties_committed', 'away_penalties_committed', 'home_games', 'home_losses', 'home_goal_diff', 'home_first_xg', 'home_first_toi', 'home_first_eff', 'home_second_xg', 'home_second_toi', 'home_second_eff', 'home_offensive_disparity', 'away_games', 'away_wins', 'away_losses', 'away_goal_diff', 'away_win_rate', 'away_first_xg', 'away_first_toi', 'away_first_eff', 'away_second_xg', 'away_second_toi', 'away_second_eff', 'away_offensive_disparity']\n",
      "\n",
      "‚úÖ Original baseline AUC: 0.6900\n",
      "‚úÖ Data loaded correctly\n",
      "Training: (1049, 28)\n",
      "Test: (263, 28)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Load original baseline artifacts\n",
    "scaler_original = pickle.load(open(\"../outputs/baseline_scaler.pkl\", \"rb\"))\n",
    "model_original = pickle.load(open(\"../outputs/baseline_model.pkl\", \"rb\"))\n",
    "\n",
    "# Get EXACT features the original baseline used\n",
    "feature_cols = list(scaler_original.feature_names_in_)\n",
    "print(f\"Using {len(feature_cols)} features from original baseline:\")\n",
    "print(feature_cols)\n",
    "\n",
    "# Load model input\n",
    "model_input = pd.read_csv(\"../data/processed/model_input.csv\")\n",
    "\n",
    "# Use ONLY the 28 features\n",
    "X = model_input[feature_cols].fillna(0)\n",
    "y = model_input['home_win']\n",
    "\n",
    "# Same split as original\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Use the ORIGINAL scaler\n",
    "X_train_scaled = scaler_original.transform(X_train)\n",
    "X_test_scaled = scaler_original.transform(X_test)\n",
    "\n",
    "# Verify original baseline still works\n",
    "baseline_pred = model_original.predict_proba(X_test_scaled)[:, 1]\n",
    "baseline_auc = roc_auc_score(y_test, baseline_pred)\n",
    "\n",
    "print(f\"\\n‚úÖ Original baseline AUC: {baseline_auc:.4f}\")\n",
    "print(f\"‚úÖ Data loaded correctly\")\n",
    "print(f\"Training: {X_train.shape}\")\n",
    "print(f\"Test: {X_test.shape}\")\n",
    "\n",
    "# Store\n",
    "models = {}\n",
    "predictions = {}\n",
    "auc_scores = {}\n",
    "scaler = scaler_original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf0d31f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL 1: LOGISTIC REGRESSION\n",
      "============================================================\n",
      "\n",
      "‚úÖ AUC: 0.6900\n",
      "\n",
      "Top 5 features:\n",
      "                 feature  coefficient\n",
      "           away_first_xg     0.597675\n",
      "away_offensive_disparity    -0.574454\n",
      "              away_shots    -0.546656\n",
      "             home_losses    -0.520231\n",
      "          away_second_xg    -0.479761\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MODEL 1: LOGISTIC REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train\n",
    "lr = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    C=1.0,  # Regularization strength\n",
    "    random_state=42\n",
    ")\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "pred_lr = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "auc_lr = roc_auc_score(y_test, pred_lr)\n",
    "\n",
    "# Save\n",
    "models['Logistic'] = {'model': lr, 'needs_scaling': True}\n",
    "predictions['Logistic'] = pred_lr\n",
    "auc_scores['Logistic'] = auc_lr\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'coefficient': lr.coef_[0]\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(f\"\\n‚úÖ AUC: {auc_lr:.4f}\")\n",
    "print(f\"\\nTop 5 features:\")\n",
    "print(feature_importance.head().to_string(index=False))\n",
    "print(\"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "406e95ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL 2: RANDOM FOREST\n",
      "============================================================\n",
      "\n",
      "‚úÖ AUC: 0.6747\n",
      "Improvement over Logistic: -0.0153\n",
      "\n",
      "Top 5 features:\n",
      "                 feature  importance\n",
      "                 away_xg    0.137498\n",
      "              away_shots    0.085912\n",
      "                 home_xg    0.085168\n",
      "home_penalties_committed    0.069544\n",
      "              home_shots    0.050195\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MODEL 2: RANDOM FOREST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,      # Number of trees\n",
    "    max_depth=8,           # Tree depth\n",
    "    min_samples_split=20,  # Min samples to split\n",
    "    min_samples_leaf=10,   # Min samples per leaf\n",
    "    max_features='sqrt',   # Features per split\n",
    "    random_state=42,\n",
    "    n_jobs=-1              # Use all CPU cores\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "pred_rf = rf.predict_proba(X_test)[:, 1]\n",
    "auc_rf = roc_auc_score(y_test, pred_rf)\n",
    "\n",
    "# Save\n",
    "models['RandomForest'] = {'model': rf, 'needs_scaling': False}\n",
    "predictions['RandomForest'] = pred_rf\n",
    "auc_scores['RandomForest'] = auc_rf\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\n‚úÖ AUC: {auc_rf:.4f}\")\n",
    "print(f\"Improvement over Logistic: {auc_rf - auc_lr:+.4f}\")\n",
    "print(f\"\\nTop 5 features:\")\n",
    "print(feature_importance.head().to_string(index=False))\n",
    "print(\"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72d95eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL 3: XGBOOST\n",
      "============================================================\n",
      "\n",
      "‚úÖ AUC: 0.6586\n",
      "Improvement over Logistic: -0.0314\n",
      "Improvement over RandomForest: -0.0161\n",
      "\n",
      "Top 5 features:\n",
      "                 feature  importance\n",
      "           away_win_rate    0.078207\n",
      "                 away_xg    0.048013\n",
      "               away_wins    0.045788\n",
      "         home_second_eff    0.045492\n",
      "home_penalties_committed    0.044027\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MODEL 3: XGBOOST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Train\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=200,       # Number of boosting rounds\n",
    "    max_depth=5,            # Tree depth\n",
    "    learning_rate=0.05,     # Shrinkage (lower = more conservative)\n",
    "    subsample=0.8,          # Row sampling per tree\n",
    "    colsample_bytree=0.8,   # Column sampling per tree\n",
    "    min_child_weight=3,     # Minimum samples per leaf\n",
    "    gamma=0.1,              # Regularization\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "pred_xgb = xgb.predict_proba(X_test)[:, 1]\n",
    "auc_xgb = roc_auc_score(y_test, pred_xgb)\n",
    "\n",
    "# Save\n",
    "models['XGBoost'] = {'model': xgb, 'needs_scaling': False}\n",
    "predictions['XGBoost'] = pred_xgb\n",
    "auc_scores['XGBoost'] = auc_xgb\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\n‚úÖ AUC: {auc_xgb:.4f}\")\n",
    "print(f\"Improvement over Logistic: {auc_xgb - auc_lr:+.4f}\")\n",
    "print(f\"Improvement over RandomForest: {auc_xgb - auc_rf:+.4f}\")\n",
    "print(f\"\\nTop 5 features:\")\n",
    "print(feature_importance.head().to_string(index=False))\n",
    "print(\"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95394a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL 4: LIGHTGBM\n",
      "============================================================\n",
      "\n",
      "‚úÖ AUC: 0.6273\n",
      "Improvement over Logistic: -0.0626\n",
      "Improvement over XGBoost: -0.0313\n",
      "\n",
      "Top 5 features:\n",
      "                 feature  importance\n",
      "                 away_xg         352\n",
      "                 home_xg         346\n",
      "              away_shots         188\n",
      "              home_shots         183\n",
      "away_penalties_committed         158\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MODEL 4: LIGHTGBM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Train\n",
    "lgb = LGBMClassifier(\n",
    "    n_estimators=200,       # Number of boosting rounds\n",
    "    max_depth=5,            # Tree depth\n",
    "    learning_rate=0.05,     # Shrinkage\n",
    "    num_leaves=31,          # Max leaves per tree\n",
    "    subsample=0.8,          # Row sampling\n",
    "    colsample_bytree=0.8,   # Column sampling\n",
    "    min_child_samples=20,   # Min samples per leaf\n",
    "    reg_alpha=0.1,          # L1 regularization\n",
    "    reg_lambda=0.1,         # L2 regularization\n",
    "    random_state=42,\n",
    "    verbose=-1              # Suppress output\n",
    ")\n",
    "lgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "pred_lgb = lgb.predict_proba(X_test)[:, 1]\n",
    "auc_lgb = roc_auc_score(y_test, pred_lgb)\n",
    "\n",
    "# Save\n",
    "models['LightGBM'] = {'model': lgb, 'needs_scaling': False}\n",
    "predictions['LightGBM'] = pred_lgb\n",
    "auc_scores['LightGBM'] = auc_lgb\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': lgb.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\n‚úÖ AUC: {auc_lgb:.4f}\")\n",
    "print(f\"Improvement over Logistic: {auc_lgb - auc_lr:+.4f}\")\n",
    "print(f\"Improvement over XGBoost: {auc_lgb - auc_xgb:+.4f}\")\n",
    "print(f\"\\nTop 5 features:\")\n",
    "print(feature_importance.head().to_string(index=False))\n",
    "print(\"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f45ab0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL 5: CATBOOST\n",
      "============================================================\n",
      "\n",
      "‚úÖ AUC: 0.6635\n",
      "Improvement over Logistic: -0.0265\n",
      "Improvement over LightGBM: +0.0361\n",
      "\n",
      "Top 5 features:\n",
      "                 feature  importance\n",
      "                 away_xg   10.302674\n",
      "home_penalties_committed    7.918733\n",
      "              away_shots    7.305247\n",
      "away_penalties_committed    7.010467\n",
      "                 home_xg    6.527714\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MODEL 5: CATBOOST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Train\n",
    "cat = CatBoostClassifier(\n",
    "    iterations=200,         # Number of boosting rounds\n",
    "    depth=5,                # Tree depth\n",
    "    learning_rate=0.05,     # Shrinkage\n",
    "    l2_leaf_reg=3,          # L2 regularization\n",
    "    random_seed=42,\n",
    "    verbose=False           # Suppress output\n",
    ")\n",
    "cat.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "pred_cat = cat.predict_proba(X_test)[:, 1]\n",
    "auc_cat = roc_auc_score(y_test, pred_cat)\n",
    "\n",
    "# Save\n",
    "models['CatBoost'] = {'model': cat, 'needs_scaling': False}\n",
    "predictions['CatBoost'] = pred_cat\n",
    "auc_scores['CatBoost'] = auc_cat\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': cat.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\n‚úÖ AUC: {auc_cat:.4f}\")\n",
    "print(f\"Improvement over Logistic: {auc_cat - auc_lr:+.4f}\")\n",
    "print(f\"Improvement over LightGBM: {auc_cat - auc_lgb:+.4f}\")\n",
    "print(f\"\\nTop 5 features:\")\n",
    "print(feature_importance.head().to_string(index=False))\n",
    "print(\"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46297647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL PERFORMANCE COMPARISON\n",
      "============================================================\n",
      "       Model      AUC  Improvement  Rank\n",
      "    Logistic 0.689953     0.000000     1\n",
      "RandomForest 0.674677    -0.015276     2\n",
      "    CatBoost 0.663455    -0.026498     3\n",
      "     XGBoost 0.658578    -0.031375     4\n",
      "    LightGBM 0.627321    -0.062632     5\n",
      "============================================================\n",
      "\n",
      "üèÜ BEST MODEL: Logistic (AUC: 0.6900)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': list(auc_scores.keys()),\n",
    "    'AUC': list(auc_scores.values())\n",
    "}).sort_values('AUC', ascending=False)\n",
    "\n",
    "results['Improvement'] = results['AUC'] - auc_lr\n",
    "results['Rank'] = range(1, len(results) + 1)\n",
    "\n",
    "print(results.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_model = results.iloc[0]['Model']\n",
    "best_auc = results.iloc[0]['AUC']\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model} (AUC: {best_auc:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "32a40e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ENSEMBLE CREATION\n",
      "============================================================\n",
      "\n",
      "Optimal weights:\n",
      "  Logistic        ‚Üí 0.340\n",
      "  RandomForest    ‚Üí 0.370\n",
      "  XGBoost         ‚Üí 0.006\n",
      "  LightGBM        ‚Üí 0.001\n",
      "  CatBoost        ‚Üí 0.283\n",
      "\n",
      "‚úÖ ENSEMBLE AUC: 0.6848\n",
      "   Best single: 0.6900\n",
      "   Improvement: -0.0052\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENSEMBLE CREATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Create prediction matrix\n",
    "pred_matrix = np.column_stack([predictions[m] for m in auc_scores.keys()])\n",
    "\n",
    "# Optimization function\n",
    "def ensemble_auc(weights):\n",
    "    weights = np.abs(weights)\n",
    "    weights = weights / weights.sum()\n",
    "    ensemble = pred_matrix @ weights\n",
    "    return -roc_auc_score(y_test, ensemble)\n",
    "\n",
    "# Optimize weights\n",
    "initial_weights = np.ones(len(auc_scores)) / len(auc_scores)\n",
    "result = minimize(ensemble_auc, initial_weights, method='Nelder-Mead')\n",
    "\n",
    "# Get optimal weights\n",
    "optimal_weights = np.abs(result.x)\n",
    "optimal_weights = optimal_weights / optimal_weights.sum()\n",
    "\n",
    "print(\"\\nOptimal weights:\")\n",
    "for model_name, weight in zip(auc_scores.keys(), optimal_weights):\n",
    "    print(f\"  {model_name:15} ‚Üí {weight:.3f}\")\n",
    "\n",
    "# Ensemble prediction\n",
    "ensemble_pred = pred_matrix @ optimal_weights\n",
    "auc_ensemble = roc_auc_score(y_test, ensemble_pred)\n",
    "\n",
    "print(f\"\\n‚úÖ ENSEMBLE AUC: {auc_ensemble:.4f}\")\n",
    "print(f\"   Best single: {best_auc:.4f}\")\n",
    "print(f\"   Improvement: {auc_ensemble - best_auc:+.4f}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8e1e001d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILTERING TO BEST MODELS ONLY\n",
      "============================================================\n",
      "Models above baseline (0.6900):\n",
      "\n",
      "Dropped models:\n",
      "  ‚ùå Logistic        ‚Üí 0.6900\n",
      "  ‚ùå RandomForest    ‚Üí 0.6747\n",
      "  ‚ùå XGBoost         ‚Üí 0.6586\n",
      "  ‚ùå LightGBM        ‚Üí 0.6273\n",
      "  ‚ùå CatBoost        ‚Üí 0.6635\n",
      "\n",
      "‚ö†Ô∏è Only 1 model above baseline - use single best model\n"
     ]
    }
   ],
   "source": [
    "print(\"FILTERING TO BEST MODELS ONLY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Keep only models better than baseline\n",
    "baseline_auc = 0.69\n",
    "best_models = {k: v for k, v in auc_scores.items() if v >= baseline_auc}\n",
    "\n",
    "print(f\"Models above baseline ({baseline_auc:.4f}):\")\n",
    "for model_name, auc in best_models.items():\n",
    "    print(f\"  ‚úÖ {model_name:15} ‚Üí {auc:.4f}\")\n",
    "\n",
    "print(f\"\\nDropped models:\")\n",
    "for model_name, auc in auc_scores.items():\n",
    "    if auc < baseline_auc:\n",
    "        print(f\"  ‚ùå {model_name:15} ‚Üí {auc:.4f}\")\n",
    "\n",
    "# Re-optimize with only best models\n",
    "if len(best_models) > 1:\n",
    "    best_pred_matrix = np.column_stack([predictions[m] for m in best_models.keys()])\n",
    "    \n",
    "    def best_ensemble_auc(weights):\n",
    "        weights = np.abs(weights)\n",
    "        weights = weights / weights.sum()\n",
    "        ensemble = best_pred_matrix @ weights\n",
    "        return -roc_auc_score(y_test, ensemble)\n",
    "    \n",
    "    initial = np.ones(len(best_models)) / len(best_models)\n",
    "    result = minimize(best_ensemble_auc, initial, method='Nelder-Mead')\n",
    "    \n",
    "    best_weights = np.abs(result.x)\n",
    "    best_weights = best_weights / best_weights.sum()\n",
    "    \n",
    "    print(\"\\nOptimal weights (best models only):\")\n",
    "    for model_name, weight in zip(best_models.keys(), best_weights):\n",
    "        print(f\"  {model_name:15} ‚Üí {weight:.3f}\")\n",
    "    \n",
    "    best_ensemble_pred = best_pred_matrix @ best_weights\n",
    "    auc_best_ensemble = roc_auc_score(y_test, best_ensemble_pred)\n",
    "    \n",
    "    print(f\"\\n‚úÖ FILTERED ENSEMBLE AUC: {auc_best_ensemble:.4f}\")\n",
    "    print(f\"   Original ensemble: {auc_ensemble:.4f}\")\n",
    "    print(f\"   Improvement: {auc_best_ensemble - auc_ensemble:+.4f}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Only 1 model above baseline - use single best model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8470e080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: outputs/ensemble_model.pkl\n",
      "\n",
      "Package contains:\n",
      "  - 5 trained models\n",
      "  - Optimal weights\n",
      "  - Feature scaler\n",
      "  - Ensemble AUC: 0.6848\n"
     ]
    }
   ],
   "source": [
    "# Package ensemble\n",
    "ensemble_package = {\n",
    "    'models': models,\n",
    "    'weights': dict(zip(auc_scores.keys(), optimal_weights)),\n",
    "    'scaler': scaler,\n",
    "    'feature_cols': feature_cols,\n",
    "    'auc': auc_ensemble,\n",
    "    'individual_aucs': auc_scores\n",
    "}\n",
    "\n",
    "with open('../outputs/ensemble_model.pkl', 'wb') as f:\n",
    "    pickle.dump(ensemble_package, f)\n",
    "\n",
    "print(\"‚úÖ Saved: outputs/ensemble_model.pkl\")\n",
    "print(f\"\\nPackage contains:\")\n",
    "print(f\"  - {len(models)} trained models\")\n",
    "print(f\"  - Optimal weights\")\n",
    "print(f\"  - Feature scaler\")\n",
    "print(f\"  - Ensemble AUC: {auc_ensemble:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef8419de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INDIVIDUAL MODEL PERFORMANCE:\n",
      "============================================================\n",
      "‚ö†Ô∏è Logistic        AUC: 0.6900  (-0.0000)\n",
      "‚ö†Ô∏è RandomForest    AUC: 0.6747  (-0.0153)\n",
      "‚ö†Ô∏è XGBoost         AUC: 0.6586  (-0.0314)\n",
      "‚ö†Ô∏è LightGBM        AUC: 0.6273  (-0.0627)\n",
      "‚ö†Ô∏è CatBoost        AUC: 0.6635  (-0.0265)\n",
      "============================================================\n",
      "Ensemble AUC: 0.6848\n",
      "Baseline (Logistic): 0.6900\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"INDIVIDUAL MODEL PERFORMANCE:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name, auc in auc_scores.items():\n",
    "    diff = auc - 0.69\n",
    "    symbol = \"‚úÖ\" if auc > 0.69 else \"‚ö†Ô∏è\"\n",
    "    print(f\"{symbol} {model_name:15} AUC: {auc:.4f}  ({diff:+.4f})\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Ensemble AUC: {auc_ensemble:.4f}\")\n",
    "print(f\"Baseline (Logistic): 0.6900\")\n",
    "print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
